{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### count vectorization example"
      ],
      "metadata": {
        "id": "jInYl59o7067"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR6x0m2p7qrZ",
        "outputId": "9360fe1e-e461-4897-9e7d-73f7bdb98e6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 11.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 39.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 42.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.13.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 34.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence_transformers) (3.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.9.24)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=ac9af4fff07a97b342081f1e5a6dba116daebe55b3a3cee9ae7b682cf51a4480\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.10.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.1 transformers-4.23.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import InputExample, SentenceTransformer, models, losses, evaluation, util\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "AtlbC1uR7wI_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question1 = 'What is the most populous state in the USA?'\n",
        "question2 = 'Which state in the United States has the most people?'"
      ],
      "metadata": {
        "id": "tjL889x-8EF6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "sample = [question1, question2]"
      ],
      "metadata": {
        "id": "fVB39E3t8OLg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(number):\n",
        "    return float(number)"
      ],
      "metadata": {
        "id": "beQnR0Zo9Yun"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit = vectorizer.fit_transform(sample).todense()\n",
        "vector1 = fit[0].tolist()[0]\n",
        "vector2 = fit[1].tolist()[0]\n",
        "float_numbers_iterator1 = map(f, vector1)\n",
        "float_numbers_iterator2 = map(f, vector2)\n",
        "vector1 = list(float_numbers_iterator1)\n",
        "vector2 = list(float_numbers_iterator2)\n",
        "words = vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "tIUnwVlB8uIU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(words)\n",
        "print(vector1)\n",
        "print(vector2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev5GEjqZ8-VX",
        "outputId": "871de06e-f47b-4bb3-91af-547b451c05cc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['has' 'in' 'is' 'most' 'people' 'populous' 'state' 'states' 'the'\n",
            " 'united' 'usa' 'what' 'which']\n",
            "[0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.0]\n",
            "[1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similarity1 = util.cos_sim(vector1, vector2)\n",
        "print(similarity1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI_kthjWFQOr",
        "outputId": "5b789d79-0fc4-44b6-cc01-4be61fdc7480"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6093]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract noun\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def get_noun(s: str):\n",
        "    doc = nlp(s)\n",
        "    if doc[0].pos_ == 'NOUN':\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "Wf897kazAJJz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector3 = vector1\n",
        "vector4 = vector2\n",
        "for j in range(len(words)):\n",
        "    pro = get_noun(words[j])\n",
        "    if pro==0:\n",
        "        vector3[j]=0.0\n",
        "        vector4[j]=0.0"
      ],
      "metadata": {
        "id": "DK4eX54LAUul"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(words)\n",
        "print(vector3)\n",
        "print(vector4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAFknDTcA3ze",
        "outputId": "d6526c04-6c53-4038-f793-70cee97eb094"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['has' 'in' 'is' 'most' 'people' 'populous' 'state' 'states' 'the'\n",
            " 'united' 'usa' 'what' 'which']\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the similarity\n",
        "similarity2 = util.cos_sim(vector3, vector4)\n",
        "print(similarity2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRTXC_gAA7I9",
        "outputId": "305faa64-7694-454e-a8d6-51cfdab170a3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5774]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### influence of typo"
      ],
      "metadata": {
        "id": "g7w7YKVjIi7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question3 = 'What is the most populous stete in the USA?'  # change \"state\" into \"stete\" \n",
        "question4 = 'Which state in the United States has the most people?'\n",
        "vectorizer = CountVectorizer()\n",
        "sample = [question3, question4]"
      ],
      "metadata": {
        "id": "riMBokQzHFFN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit = vectorizer.fit_transform(sample).todense()\n",
        "vector1 = fit[0].tolist()[0]\n",
        "vector2 = fit[1].tolist()[0]\n",
        "float_numbers_iterator1 = map(f, vector1)\n",
        "float_numbers_iterator2 = map(f, vector2)\n",
        "vector1 = list(float_numbers_iterator1)\n",
        "vector2 = list(float_numbers_iterator2)\n",
        "words = vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "peJmOAXfJ3jP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(words)\n",
        "print(vector1)\n",
        "print(vector2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxANVBdzJ8HM",
        "outputId": "3adc0ef2-de77-403b-e693-5aaf29a0fa37"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['has' 'in' 'is' 'most' 'people' 'populous' 'state' 'states' 'stete' 'the'\n",
            " 'united' 'usa' 'what' 'which']\n",
            "[0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 2.0, 0.0, 1.0, 1.0, 0.0]\n",
            "[1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 2.0, 1.0, 0.0, 0.0, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similarity3 = util.cos_sim(vector1, vector2)\n",
        "print(similarity3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sx4xZfr_KV0U",
        "outputId": "66b8cd30-5664-4830-f944-c542a63b4cd6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5222]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### the performance on abbriviation"
      ],
      "metadata": {
        "id": "uqJms1x0K2eO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question5 = 'What is the most populous state in the USA?'\n",
        "question6 = 'Which state in the USA has the most people?' # change \"United State\" to \"USA\"\n",
        "vectorizer = CountVectorizer()\n",
        "sample = [question5, question6]"
      ],
      "metadata": {
        "id": "T0_AclXdKt7y"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit = vectorizer.fit_transform(sample).todense()\n",
        "vector1 = fit[0].tolist()[0]\n",
        "vector2 = fit[1].tolist()[0]\n",
        "float_numbers_iterator1 = map(f, vector1)\n",
        "float_numbers_iterator2 = map(f, vector2)\n",
        "vector1 = list(float_numbers_iterator1)\n",
        "vector2 = list(float_numbers_iterator2)\n",
        "words = vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "HGxZpDEOLNDk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(words)\n",
        "print(vector1)\n",
        "print(vector2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juIOr3VwLPC6",
        "outputId": "b2a52999-2adb-46c8-908e-82f019df38b7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['has' 'in' 'is' 'most' 'people' 'populous' 'state' 'the' 'usa' 'what'\n",
            " 'which']\n",
            "[0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0]\n",
            "[1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similarity3 = util.cos_sim(vector1, vector2)\n",
        "print(similarity3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riDGYryQLQw4",
        "outputId": "a7071b63-fabc-4a57-a9f8-e707be000bdf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.7273]])\n"
          ]
        }
      ]
    }
  ]
}